{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60658c85-1329-4cfc-95aa-8ad07b60491b",
   "metadata": {},
   "source": [
    "Q-Learning\n",
    "--\n",
    "\n",
    "1. Load the environment and other components for RL from “OpenAI Gym” library for \"Self Driving Cab\" optimized decision making task.\n",
    "2. Apply Q-Learning algorithm to do optimization of \"Self Driving Cab\" task. Obtain the performance metrics\n",
    "3. Compare the performance with and without Q-Learning algorithm. \n",
    "4. Give inference. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edf39a-9282-4702-9b43-b0094bc8f288",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61540e9b-14ff-470a-92b2-34523b9714a5",
   "metadata": {},
   "source": [
    "**REWARDS:**\n",
    "- The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired. \n",
    "- The agent should be penalized if it tries to drop off a passenger in wrong locations. \n",
    "- The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a4bb02-e7d5-45fc-88e2-d3c0ca09c1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting cmake\n",
      "  Downloading cmake-3.31.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: scipy in /home/aaliya/.local/lib/python3.8/site-packages (1.10.1)\n",
      "Collecting gym[atari]\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m928.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /home/aaliya/.local/lib/python3.8/site-packages (from gym[atari]) (1.24.4)\n",
      "Collecting cloudpickle>=1.2.0 (from gym[atari])\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gym-notices>=0.0.4 (from gym[atari])\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari]) (7.1.0)\n",
      "Collecting ale-py~=0.8.0 (from gym[atari])\n",
      "  Downloading ale_py-0.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.8.0->gym[atari]) (6.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.8.0->gym[atari]) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.18.1)\n",
      "Downloading cmake-3.31.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m952.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ale_py-0.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m943.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827616 sha256=7b6b1ae70f6ba9a522f889cd871b80c6a500109c2a5e51546431816605bf9661\n",
      "  Stored in directory: /home/aaliya/.cache/pip/wheels/17/79/65/7afedc162d858b02708a3b8f7a6dd5b1000dcd5b0f894f7cc1\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, cmake, cloudpickle, gym, ale-py\n",
      "Successfully installed ale-py-0.8.1 cloudpickle-3.1.0 cmake-3.31.0.1 gym-0.26.2 gym-notices-0.0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb91aba-975d-4d72-be92-98432dadb029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a70c2f-6492-4f2f-a108-c229aff812ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")  #render mode: human, ansi, rgb_array\n",
    "#reset the environment and render it\n",
    "state = env.reset()\n",
    "#render the env as ansi text with rgb\n",
    "ansi_output = env.render()\n",
    "print(ansi_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ce515-b22a-4809-a341-58900f3db9df",
   "metadata": {},
   "source": [
    "Problem statement from GYM documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7430bf-b8a5-4d76-8dd2-26efc8c5a144",
   "metadata": {},
   "source": [
    "\"\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c87e1e1-4c81-4466-bd6b-752351f60307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa48bf-0318-435a-baff-650bc43d550c",
   "metadata": {},
   "source": [
    "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2744af6-b454-41f2-bc8b-a2e3a4104b98",
   "metadata": {},
   "source": [
    "- 0 = south\n",
    "- 1 = north\n",
    "- 2 = east\n",
    "- 3 = west\n",
    "- 4 = pickup\n",
    "- 5 = dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e59c1590-8a96-46be-b7c3-0f92258d7c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3807e302-1b8a-4df8-8dc8-321f4cce7dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REWARD TABLE\n",
    "env.P[328]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65815a0a-0890-4683-a81b-f4a6ced87306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing QLearning\n",
    "#Initializing Q-table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b175632-4596-4653-8175-6c21865b6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc311723-0baa-4725-9e79-7ce4f94192a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_episodes = 1000\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]  # Get initial state\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Exploration vs. exploitation\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])  # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)  # Take the action\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        # Update Q-value for current state-action pair\n",
    "        q_table[state, action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "\n",
    "        # Track penalties\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "\n",
    "    all_epochs.append(epochs)\n",
    "    all_penalties.append(penalties)\n",
    "\n",
    "print(\"Training finished.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c90f883-1803-4311-845b-35637dfc09cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.33546881, -2.32464586, -2.32769383, -2.33001073, -6.13422566,\n",
       "       -4.40788394])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d382ca47-21e6-4280-8eb2-393d58ec235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_47208/3641414001.py\", line 6, in <module>\n",
      "    state = env.reset()[0]  # Extract the state value from the reset output\n",
      "NameError: name 'env' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1030, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/IPython/core/ultratb.py\", line 1082, in get_records\n",
      "    style = stack_data.style_with_executing_node(style, self._tb_highlight)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/stack_data/core.py\", line 455, in style_with_executing_node\n",
      "    class NewStyle(style):\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 91, in __new__\n",
      "    ndef[4] = colorformat(styledef[3:])\n",
      "  File \"/usr/lib/python3/dist-packages/pygments/style.py\", line 58, in colorformat\n",
      "    assert False, \"wrong color format %r\" % text\n",
      "AssertionError: wrong color format 'ansiyellow'\n"
     ]
    }
   ],
   "source": [
    "# Evaluate agent's performance after Q-learning\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 10\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()[0]  # Extract the state value from the reset output\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  # Choose the best action based on Q-table\n",
    "        next_state, reward, done, _, _ = env.step(action)  # Take action\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state  # Update the state\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes with Q-Learning:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed9ab0-2d85-4cc5-bd3a-3286f9553a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
